{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import sys\n",
    "from torch.utils.data import Dataset\n",
    "import numpy as np\n",
    "sys.path.append('/home3/ebrahim/isr/isr_model_review/BP06/')\n",
    "from utils import cosine_sim\n",
    "from datasets import OneHotLetters, OneHotLetters_test\n",
    "from run_test_trials import run_test_trials\n",
    "from simulation_one import simulation_one\n",
    "from RNNcell import RNN_one_layer\n",
    "import wandb\n",
    "device = torch.device(\"cpu\")\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "import statsmodels\n",
    "import pandas as pd\n",
    "from statsmodels.stats.anova import AnovaRM\n",
    "import scipy.stats as stats\n",
    "from scipy.stats import ttest_rel\n",
    "import pickle\n",
    "%matplotlib inline\n",
    "from basic_model import load_model_basic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class creating_y_hat(load_model_basic):\n",
    "\n",
    "    def __init__(self, base):\n",
    "\n",
    "        super().__init__(base)\n",
    "\n",
    "    def y_hat_transpositions(self):\n",
    "\n",
    "        transpositions_test = self.sim_one.transpositions_test\n",
    "        ll = transpositions_test.shape[0]\n",
    "\n",
    "        for i in range(ll):\n",
    "            hist, bins = np.histogram(transpositions_test[i, :], bins=np.arange(1,ll+2,1), density=True)\n",
    "\n",
    "            print(np.round(hist,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# information for loading model\n",
    "base = '/home3/ebrahim/isr/isr_model_review/BP06/'\n",
    "modelPATH_arr = ['stateful_True_noise_0.0_opt_Adam_no_grad_num_letters_26_nl_test_26_True_']\n",
    "wandB_arr = [\"ehrdds6f\", \"ed8798k6\", \"4ko6rcqt\", \"9s6t7l9o\", \"iwxesev8\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_yhat = creating_y_hat(base)\n",
    "create_yhat.load_model('/ebrahimfeghhi/BP06/' + wandB_arr[0], modelPATH_arr[0] + '1')\n",
    "create_yhat.run_simulation_one(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.94 0.06 0.   0.   0.   0.  ]\n",
      "[0.06 0.86 0.08 0.   0.   0.  ]\n",
      "[0.   0.07 0.84 0.08 0.   0.  ]\n",
      "[0.   0.01 0.07 0.82 0.09 0.  ]\n",
      "[0.   0.   0.01 0.07 0.77 0.13]\n",
      "[0.   0.   0.01 0.02 0.13 0.84]\n"
     ]
    }
   ],
   "source": [
    "create_yhat.y_hat_transpositions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OneHotLetters_EB(Dataset):\n",
    "  \n",
    "    def __init__(self, max_length, num_cycles, test_path, num_classes, batch_size=1, num_letters=26, \n",
    "                delay_start=0, delay_middle=0, double_trial=False):\n",
    "\n",
    "        \"\"\" Initialize class to generate letters, represented as one hot vectors in 26 dimensional space. \n",
    "        :param int max_length: maximum number of letters \n",
    "        :param int num_cycles: number of cycles (1 cycle = set of lists of length 1,...,max_length)\n",
    "        :param int num_classes: number of classes \n",
    "        :param int batch_size: size of each batch\n",
    "        :param int num_letters: number of letters in vocabulary \n",
    "        :param str test_path: path to test_list (type should be set for quick look up)\n",
    "        :param float repeat_prob: fraction of trials to sample with repetition\n",
    "        :param int delay_start: how much delay before trial starts \n",
    "        :param int delay_middle: how much delay between retrieval and recall \n",
    "        :param bool double_trial: if true, one list contains two trials \n",
    "        \"\"\" \n",
    "\n",
    "        self.max_length = max_length\n",
    "        self.num_letters = num_letters\n",
    "        self.num_cycles = num_cycles\n",
    "        self.num_classes = num_classes\n",
    "        self.batch_size = batch_size\n",
    "        self.storage = []\n",
    "        self.delay_start = delay_start\n",
    "        self.delay_middle = delay_middle\n",
    "        self.list_length = 6 \n",
    "        self.double_trial = double_trial\n",
    "\n",
    "        with open(test_path, 'rb') as f:\n",
    "            self.test_data = pickle.load(f)\n",
    "    \n",
    "    def __len__(self):\n",
    "\n",
    "        return self.num_cycles * self.max_length\n",
    "\n",
    "    def construct_trial(self): \n",
    "\n",
    "        '''\n",
    "        Generates a training example\n",
    "        '''\n",
    "\n",
    "        rng = np.random.default_rng()\n",
    "\n",
    "        delay_start = np.ones(self.delay_start) * (self.num_letters+1)\n",
    "        delay_middle = np.ones(self.delay_middle) * (self.num_letters+1)\n",
    "     \n",
    "        letters = rng.choice(self.num_letters, self.list_length, replace=False) \n",
    "            \n",
    "        # ensure train and test are not overlapping\n",
    "        if self.list_length > 1: \n",
    "            while tuple(letters) in self.test_data[str(self.list_length)]:\n",
    "                rng = np.random.default_rng()\n",
    "                letters = rng.choice(self.num_letters, self.list_length, replace=False) \n",
    "                    \n",
    "        recall_cue = np.ones(self.list_length+1) * self.num_letters \n",
    "\n",
    "        X = torch.nn.functional.one_hot(torch.from_numpy(\n",
    "            np.hstack((delay_start, letters, delay_middle, recall_cue))).to(torch.long),\n",
    "        num_classes=self.num_classes)\n",
    "\n",
    "        # output is letters during letter presentation\n",
    "        # letters again after recall cue\n",
    "        # and finally end of list cue \n",
    "        y = torch.from_numpy(np.hstack((delay_start, letters, delay_middle,\n",
    "        letters, self.num_letters))).to(torch.long)\n",
    "        self.letters_to_prob_dist(letters)\n",
    "        y = torch.nn.functional.one_hot(y, num_classes=self.num_classes)\n",
    "\n",
    "        return X, y\n",
    "\n",
    "    def letters_to_prob_dist(self, letters):\n",
    "\n",
    "        '''\n",
    "        Converts letters to a probability distribution, specified by human error patterns. \n",
    "        '''\n",
    "\n",
    "        print(letters)\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        # every new batch, increment the list_length \n",
    "        # once the list length exceeds the max length, return back to 1 \n",
    "        if idx % self.batch_size == 0: \n",
    "            self.list_length += 1\n",
    "            if self.list_length > self.max_length:\n",
    "                self.list_length = 1\n",
    "\n",
    "        X, y = self.construct_trial()\n",
    "\n",
    "        if self.double_trial: \n",
    "\n",
    "            rng = np.random.default_rng()\n",
    "            uniform_0_1 = rng.random()\n",
    "\n",
    "            if uniform_0_1 < self.storage_frac:\n",
    "                # the second list is a fixed delay period followed by recalling the previous list \n",
    "                # these are termed storage trials\n",
    "                list_recall = self.delay_start + self.list_length + self.delay_middle\n",
    "                X2, y2 = self.recall_list_from_storage(X[list_recall:], y[list_recall:])\n",
    "            else:\n",
    "                # the second list is a normal list, i.e. presented letters followed by recalling those letters\n",
    "                X2, y2 = self.construct_trial()\n",
    "\n",
    "            X = torch.cat((X, X2),axis=0)\n",
    "            y = torch.cat((y, y2),axis=0)\n",
    "\n",
    "        return X.to(torch.float32), y.to(torch.float32) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[22  3  5 11  2 15]\n"
     ]
    }
   ],
   "source": [
    "eb = OneHotLetters_EB(9, 1, '/home3/ebrahim/isr/isr_model_review/BP06/test_set/test_lists_cleaned_26.pkl', \n",
    "                                num_classes=27)\n",
    "X, y = eb.construct_trial()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
